{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJVPWOLA0NZC",
        "outputId": "7f96fd4f-0fbb-42e4-c844-b45247ef685e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=021ece33cad5b62117496c97252a7c50bb77227e889f82141ac471910306550c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Define topics\n",
        "topics = [\n",
        "    'Data Science',\n",
        "    'Artificial Intelligence',\n",
        "    'Neural Networks',\n",
        "    'Natural Language Processing',\n",
        "    'Deep Learning',\n",
        "    'Reinforcement Learning',\n",
        "    'Big Data',\n",
        "    'Statistics',\n",
        "    'Python Programming Language',\n",
        "    'Politics',\n",
        "    'Business',\n",
        "    'Physics',\n",
        "    'Mathematics',\n",
        "    'Tesla Motors',\n",
        "    'OpenAI',\n",
        "    'C++ programming Language',\n",
        "    'Military'\n",
        "]\n",
        "\n",
        "topic_contents = []\n",
        "for topic in topics:\n",
        "    try:\n",
        "        content = wikipedia.page(topic).content\n",
        "\n",
        "        if content:\n",
        "            #Limit content to 200 words (it can be changed as per the need)\n",
        "            limited_content = ' '.join(content.split()[:200])\n",
        "            topic_contents.append(limited_content)\n",
        "        else:\n",
        "            print(f\"Empty content for {topic}. Skipping.\")\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"Page for {topic} not found.\")\n",
        "    except wikipedia.exceptions.DisambiguationError:\n",
        "        print(f\"Ambiguous page for {topic}.\")\n",
        "\n",
        "#Combine and clean text data\n",
        "combined_content = ' '.join(topic_contents)\n",
        "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', combined_content).lower()\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "tokens = tokenizer.texts_to_sequences([cleaned_text])[0]\n",
        "\n",
        "#Create sequences of input and output data\n",
        "seq_length = 30  #Limiting sequence length to 30 words\n",
        "sequences = []\n",
        "for i in range(0, len(tokens) - seq_length, 1):\n",
        "    sequences.append(tokens[i:i + seq_length + 1])\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "#Shuffle the sequences\n",
        "np.random.shuffle(sequences)\n",
        "\n",
        "#Split sequences into input and target\n",
        "X = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n",
        "\n",
        "print(\"Total number of tokens:\", len(tokens))\n",
        "print(\"Vocabulary size:\", len(tokenizer.word_index))\n",
        "\n",
        "#Check data validity\n",
        "if sequences.any():\n",
        "    print(\"Dataset shape:\", sequences.shape)\n",
        "else:\n",
        "    print(\"The dataset is empty or contains invalid data.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ptr7hEV0euF",
        "outputId": "379bc4bb-5271-450b-ba85-8520cefea90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ambiguous page for Physics.\n",
            "Total number of tokens: 3136\n",
            "Vocabulary size: 1140\n",
            "Dataset shape: (3106, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1  #Vocabulary size (+1 for padding)\n",
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, activation='tanh'),\n",
        "    tf.keras.layers.LSTM(512, return_sequences=True, activation='tanh'),\n",
        "    tf.keras.layers.LSTM(512, activation='tanh'),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')])\n",
        "\n",
        "#Compile the model\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "#Model Training\n",
        "model.fit(X, y, batch_size=64, epochs=20, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzl-u1pW1vzP",
        "outputId": "0ec7ae06-798c-4fd5-9482-034c4060abc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "39/39 [==============================] - 93s 2s/step - loss: 6.5857 - accuracy: 0.0475 - val_loss: 6.4994 - val_accuracy: 0.0450\n",
            "Epoch 2/20\n",
            "39/39 [==============================] - 87s 2s/step - loss: 6.1675 - accuracy: 0.0419 - val_loss: 6.6619 - val_accuracy: 0.0450\n",
            "Epoch 3/20\n",
            "39/39 [==============================] - 88s 2s/step - loss: 6.0767 - accuracy: 0.0495 - val_loss: 6.8237 - val_accuracy: 0.0450\n",
            "Epoch 4/20\n",
            "39/39 [==============================] - 91s 2s/step - loss: 6.0842 - accuracy: 0.0471 - val_loss: 6.8993 - val_accuracy: 0.0305\n",
            "Epoch 5/20\n",
            "39/39 [==============================] - 90s 2s/step - loss: 6.0298 - accuracy: 0.0447 - val_loss: 6.9118 - val_accuracy: 0.0450\n",
            "Epoch 6/20\n",
            "39/39 [==============================] - 90s 2s/step - loss: 6.0029 - accuracy: 0.0366 - val_loss: 6.9656 - val_accuracy: 0.0450\n",
            "Epoch 7/20\n",
            "39/39 [==============================] - 90s 2s/step - loss: 5.9732 - accuracy: 0.0491 - val_loss: 7.0267 - val_accuracy: 0.0450\n",
            "Epoch 8/20\n",
            "39/39 [==============================] - 86s 2s/step - loss: 5.9639 - accuracy: 0.0407 - val_loss: 7.0686 - val_accuracy: 0.0338\n",
            "Epoch 9/20\n",
            "39/39 [==============================] - 85s 2s/step - loss: 5.9578 - accuracy: 0.0439 - val_loss: 7.1411 - val_accuracy: 0.0450\n",
            "Epoch 10/20\n",
            "39/39 [==============================] - 87s 2s/step - loss: 5.9542 - accuracy: 0.0479 - val_loss: 7.2129 - val_accuracy: 0.0322\n",
            "Epoch 11/20\n",
            "39/39 [==============================] - 89s 2s/step - loss: 5.9528 - accuracy: 0.0475 - val_loss: 7.1800 - val_accuracy: 0.0450\n",
            "Epoch 12/20\n",
            "39/39 [==============================] - 89s 2s/step - loss: 6.0032 - accuracy: 0.0491 - val_loss: 7.1724 - val_accuracy: 0.0450\n",
            "Epoch 13/20\n",
            "39/39 [==============================] - 89s 2s/step - loss: 6.0410 - accuracy: 0.0491 - val_loss: 7.1854 - val_accuracy: 0.0450\n",
            "Epoch 14/20\n",
            "39/39 [==============================] - 89s 2s/step - loss: 6.0336 - accuracy: 0.0491 - val_loss: 7.2506 - val_accuracy: 0.0450\n",
            "Epoch 15/20\n",
            "39/39 [==============================] - 90s 2s/step - loss: 6.0308 - accuracy: 0.0491 - val_loss: 7.2253 - val_accuracy: 0.0450\n",
            "Epoch 16/20\n",
            "39/39 [==============================] - 89s 2s/step - loss: 6.0270 - accuracy: 0.0491 - val_loss: 7.2481 - val_accuracy: 0.0450\n",
            "Epoch 17/20\n",
            "39/39 [==============================] - 96s 2s/step - loss: 6.0233 - accuracy: 0.0455 - val_loss: 7.2346 - val_accuracy: 0.0450\n",
            "Epoch 18/20\n",
            "39/39 [==============================] - 89s 2s/step - loss: 6.0250 - accuracy: 0.0491 - val_loss: 7.2519 - val_accuracy: 0.0450\n",
            "Epoch 19/20\n",
            "39/39 [==============================] - 93s 2s/step - loss: 6.0225 - accuracy: 0.0491 - val_loss: 7.3024 - val_accuracy: 0.0450\n",
            "Epoch 20/20\n",
            "39/39 [==============================] - 88s 2s/step - loss: 6.0228 - accuracy: 0.0491 - val_loss: 7.3296 - val_accuracy: 0.0450\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7be1404c9ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhViCol4xS1A",
        "outputId": "01e82d7d-87e8-4541-f7af-650f45b7d566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 30, 256)           292096    \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 30, 256)           525312    \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 30, 512)           1574912   \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 512)               2099200   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1141)              585333    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5076853 (19.37 MB)\n",
            "Trainable params: 5076853 (19.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to generate text using the trained model\n",
        "def generate_text_with_temperature(topic, next_words, temperature=1.0):\n",
        "    generated_text = topic.lower()\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        predicted = model.predict(token_list, verbose=0)[-1]  # Last prediction\n",
        "        predicted = np.log(predicted) / temperature\n",
        "        exp_preds = np.exp(predicted)\n",
        "        predicted = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        predicted_index = np.random.choice(len(predicted), p=predicted)\n",
        "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
        "\n",
        "        generated_text += \" \" + output_word\n",
        "    return generated_text\n",
        "\n",
        "#Prompt the model for information related to a topic with temperature control\n",
        "topic = input('Ask me from the listed topics.....\\n')\n",
        "generated_info = generate_text_with_temperature(topic, next_words=50, temperature=1)\n",
        "print(generated_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx8sDXNM7P47",
        "outputId": "7b470e54-376b-4dd8-9687-28cfd6352eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask me from the listed topics.....\n",
            "Tesla Motors\n",
            "tesla motors diverse internal it in the of greek underlying naturallanguage forms may intelligence identify was to cannot elon premise as a and concerns games led refers as business knowledge nature nonprofit or and andin fundamental such subfield goal naturallanguage data usually a not history the range swarm programs machine learning datadriven\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from language_tool_python import LanguageTool\n",
        "\n",
        "# Assuming `generated_text` contains the output generated by your model\n",
        "generated_text = \"Your generated text here...\"\n",
        "\n",
        "# Create a LanguageTool instance\n",
        "tool = LanguageTool('en-US')\n",
        "\n",
        "# Check and correct grammar errors in the generated text\n",
        "matches = tool.check(generated_text)\n",
        "corrected_text = tool.correct(generated_info)\n",
        "\n",
        "# Print corrected text\n",
        "print(\"Original Text:\")\n",
        "print(generated_info)\n",
        "print(\"\\nCorrected Text:\")\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTEZyaM2IRFj",
        "outputId": "efe7fa62-6a64-4e19-929a-8abb6116b668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "tesla motors diverse internal it in the of greek underlying naturallanguage forms may intelligence identify was to cannot elon premise as a and concerns games led refers as business knowledge nature nonprofit or and andin fundamental such subfield goal naturallanguage data usually a not history the range swarm programs machine learning datadriven\n",
            "\n",
            "Corrected Text:\n",
            "Tesla motors diverse internal it in the of Greek underlying natural language forms may intelligence identify was to cannot Elon premise as a and concerns games led refers as business knowledge nature nonprofit or and an din fundamental such subfield goal natural language data usually a not history the range swarm programs machine learning data driven\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pe\n",
        "tokenizer_file = 'tokenizer_model.pkl'\n",
        "with open(tokenizer_file, 'wb') as token_file:\n",
        "    pe.dump(tokenizer, token_file)"
      ],
      "metadata": {
        "id": "NmH_PYsEESc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model\n",
        "model.save(\"Chat_pedia_model.h5\")\n",
        "\n",
        "#Load the model\n",
        "#loaded_model = tf.keras.models.load_model(\"Chat_pedia_model.h5\")\n"
      ],
      "metadata": {
        "id": "H8gYSXmEHgy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJDSKZEvIXrG",
        "outputId": "0c5dd584-5c5d-4de8-edf6-327ef5237625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2023.7.22)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('tokenizer_model.pkl', 'rb') as file:\n",
        "    tokenizer1 = pickle.load(file)\n"
      ],
      "metadata": {
        "id": "7Ix2Ag5L4Ndk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "tokenizer_config = tokenizer1.to_json()\n",
        "\n",
        "with open('tokenizer_config.json', 'w') as json_file:\n",
        "    json.dump(tokenizer_config, json_file)"
      ],
      "metadata": {
        "id": "jyAwZwxn7d7a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQVlWMVj49mx",
        "outputId": "1cf6fbad-e026-4380-c028-7c1e8a8175c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.preprocessing.text.Tokenizer at 0x7d574ef5a590>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}